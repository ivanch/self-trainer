{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FM-70kEuav1"
   },
   "source": [
    "# Fashion-MNIST Self-Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KV8rgh5huhIn"
   },
   "source": [
    "## Setup Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFOrFjgTuU6R"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, MaxPool2D\n",
    "from keras.layers import BatchNormalization, LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, BatchNormalization, Activation, Embedding, ZeroPadding2D, LeakyReLU, UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4Nex6p2usJl"
   },
   "source": [
    "## Setup MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vmtcZUftxVI3"
   },
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST data\n",
    "x_train = np.load(\"fashion-x.npy\")\n",
    "y_train = np.load(\"fashion-y.npy\")\n",
    "x_test = np.load(\"fashion-tx.npy\")\n",
    "y_test = np.load(\"fashion-ty.npy\")\n",
    "\n",
    "x_test = x_test.reshape((len(x_test),28,28,1))\n",
    "\n",
    "x_train = x_train.astype(np.float32)/255.0\n",
    "x_test = x_test.astype(np.float32)/255.0\n",
    "\n",
    "y = np.zeros((len(y_test),10))\n",
    "for i in range(len(y_test)):\n",
    "    y[i][y_test[i]] = 1.0\n",
    "y_test = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2sY4PgmuvGt"
   },
   "source": [
    "## Setup Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHckv_FV6Eyu"
   },
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    def __init__(self, width=28, height=28, channels=1):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channels = channels\n",
    "\n",
    "        self.shape = (self.width, self.height, self.channels)\n",
    "        self.opt = Adam(lr=0.0001)\n",
    "        \n",
    "        self.acnn = self.ACNN()\n",
    "        self.acnn.compile(optimizer=self.opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    def ACNN(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=32, kernel_size=7, strides=(3, 3), padding='same', input_shape=self.shape))\n",
    "        model.add(Dropout(rate=0.2))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(momentum=0.99))\n",
    "        model.add(Conv2D(filters=32, kernel_size=4, strides=(2, 2), padding='same'))\n",
    "        model.add(Dropout(rate=0.2))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(filters=32, kernel_size=3, strides=(1, 1), padding='same'))\n",
    "        model.add(Dropout(rate=0.1))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(filters=32, kernel_size=3, strides=(1, 1), padding='same'))\n",
    "        model.add(Dropout(rate=0.1))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(filters=32, kernel_size=3, strides=(1, 1), padding='same'))\n",
    "        model.add(Dropout(rate=0.1))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(filters=32, kernel_size=3, strides=(1, 1), padding='same'))\n",
    "        model.add(Dropout(rate=0.1))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(filters=32, kernel_size=3, strides=(1, 1), padding='same'))\n",
    "        model.add(Dropout(rate=0.1))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4096, activation='tanh'))\n",
    "        model.add(Dense(units=10, activation='softmax'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9scEurnOu3dq"
   },
   "source": [
    "## Setup Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7fTIQ0FcvDVN"
   },
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self, width=28, height=28, channels=1):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.latent_dim = 192+10\n",
    "\n",
    "        self.shape = (self.width, self.height, self.channels)\n",
    "\n",
    "        self.opt = Adam(lr=0.0002, beta_1=0.5, decay=8e-8)\n",
    "\n",
    "        self.gen = self.generator()\n",
    "        self.gen.compile(loss='binary_crossentropy', optimizer=self.opt)\n",
    "\n",
    "        self.disc = self.discriminator()\n",
    "        self.disc.compile(loss='binary_crossentropy', optimizer=self.opt, metrics=['accuracy'])\n",
    "\n",
    "        self.disc.trainable = False\n",
    "        self.stack = Sequential()\n",
    "        self.stack.add(self.gen)\n",
    "        self.stack.add(self.disc)\n",
    "        self.stack.compile(loss='binary_crossentropy', optimizer=self.opt)\n",
    "\n",
    "    def discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, kernel_size=5, strides=(2, 2), padding='same', input_shape=self.shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(64, kernel_size=5, strides=(2, 2), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=(1, 1), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=(1, 1), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=(1, 1), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "        return model\n",
    "        \n",
    "    def generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(7 * 7 * 384,input_shape=(self.latent_dim,)))\n",
    "        model.add(Reshape((7,7,384)))\n",
    "        model.add(Conv2DTranspose(64, kernel_size=3, strides=(2, 2), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(Conv2DTranspose(64, kernel_size=3, strides=(2, 2), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(Conv2DTranspose(32, kernel_size=5, strides=(1, 1), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(Conv2DTranspose(32, kernel_size=5, strides=(1, 1), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2DTranspose(self.channels, kernel_size=1, strides=(1, 1), padding='same'))\n",
    "        model.add(Activation('sigmoid'))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def train(self, x_train, y_train, epochs=20000, batch = 32):\n",
    "        for epoch in range(epochs):\n",
    "            half = int(batch/2)\n",
    "            \n",
    "            ## Train discriminator\n",
    "            random_index = np.random.randint(0, len(x_train) - batch/2)\n",
    "            real_x = x_train[random_index : random_index + half].reshape(half, self.width, self.height, self.channels)\n",
    "            real_y = y_train[random_index : random_index + half] \n",
    "            \n",
    "            noise = np.random.normal(0, 1, (half, self.latent_dim))\n",
    "            for i in range(len(noise)):\n",
    "                choose = random.randint(0,9)\n",
    "                for j in range(10):\n",
    "                    noise[i][j] = 0.0\n",
    "                noise[i][choose] = 1.0\n",
    "            syntetic = self.gen.predict(noise)\n",
    "\n",
    "            x = np.concatenate((real_x, syntetic))\n",
    "            y = np.zeros((batch,11))\n",
    "            for i in range(half):\n",
    "                y[i][int(real_y[i])] = 1.0\n",
    "            for i in range(half,batch):\n",
    "                y[i][10] = 1.0\n",
    "            \n",
    "            d_loss = self.disc.train_on_batch(x, y)\n",
    "\n",
    "            ## Train generator\n",
    "            noise = np.random.normal(0, 1, (batch, self.latent_dim))\n",
    "            for i in range(len(noise)):\n",
    "                choose = random.randint(0,9)\n",
    "                for j in range(10):\n",
    "                    noise[i][j] = 0.0\n",
    "                noise[i][choose] = 1.0\n",
    "            noise_y = self.gen.predict(noise) # Generated image\n",
    "            label = []\n",
    "            for i in range(len(noise)):\n",
    "                label.append(np.argmax(noise[i][:10]))\n",
    "            y = np.zeros((batch, 11))\n",
    "            \n",
    "            for i in range(len(y)):\n",
    "                label_y = label[i]\n",
    "                y[i][label_y] = 1\n",
    "            \n",
    "            g_loss = self.stack.train_on_batch(noise, y)\n",
    "\n",
    "            if(epoch%1000 == 0):\n",
    "                print(f'[Epoch: #{epoch}] Discriminator loss: {d_loss[0]}, Generator loss: {g_loss}')\n",
    "                \n",
    "                samples = 10\n",
    "                noise = np.random.normal(0, 1,(samples, self.latent_dim))\n",
    "                for i in range(len(noise)):\n",
    "                    for j in range(10):\n",
    "                        noise[i][j] = 0.0\n",
    "                    noise[i][i] = 1.0\n",
    "                images = self.gen.predict(noise)\n",
    "                plt.figure(figsize=(10, 10))\n",
    "\n",
    "                for i in range(images.shape[0]):\n",
    "                    plt.subplot(4, 4, i+1)\n",
    "                    image = images[i, :, :, :]\n",
    "                    image *= 255\n",
    "                    image = np.reshape(image, [self.height, self.width])\n",
    "                    plt.imshow(image, cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(\"Samples/gan_%d.png\" % epoch)\n",
    "                plt.close('all')\n",
    "\n",
    "    def genDataset(self, size):\n",
    "        noise = np.random.normal(0,1,(size, self.latent_dim))\n",
    "        y = np.zeros((size,10))\n",
    "        for i in range(size):\n",
    "            choose = random.randint(0,9)\n",
    "            for j in range(10):\n",
    "                noise[i][j] = 0.0\n",
    "            noise[i][choose] = 1.0\n",
    "            y[i][choose] = 1.0\n",
    "        x = self.gen.predict(noise)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9TXOxRd5u-sy"
   },
   "source": [
    "## Train both CNN and GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GAN and CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZJNVUHMXJ0q"
   },
   "outputs": [],
   "source": [
    "gan = GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxADBozkNdSS"
   },
   "outputs": [],
   "source": [
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oKs-mEjCXL7w"
   },
   "outputs": [],
   "source": [
    "# LOAD GAN WEIGHTS\n",
    "gan.gen.load_weights(\"generator.h5\")\n",
    "gan.disc.load_weights(\"discriminator.h5\")\n",
    "gan.stack.load_weights(\"stacked.h5\")\n",
    "\n",
    "# LOAD CNN WEIGHTS\n",
    "cnn.acnn.load_weights(\"CNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE CNN WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kk6LIDgvXbgD"
   },
   "outputs": [],
   "source": [
    "cnn.acnn.save_weights(\"CNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE GAN WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oywji_EFXXBe"
   },
   "outputs": [],
   "source": [
    "gan.gen.save_weights(\"generator.h5\")\n",
    "gan.disc.save_weights(\"discriminator.h5\")\n",
    "gan.stack.save_weights(\"stacked.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN GAN WITH MNIST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "miSj33NLXebO",
    "outputId": "bba9b428-bd88-4ac1-903d-93e9fb545f14"
   },
   "outputs": [],
   "source": [
    "gan.train(x_train[:2000],y_train[:2000], epochs=5001, batch=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN CNN WITH GAN GENERATED IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bxi5QVWrXh3s"
   },
   "outputs": [],
   "source": [
    "for e in range(10):\n",
    "    gan.train(x_train[:2000],y_train[:2000], epochs=500, batch=32)\n",
    "    for i in range(10):\n",
    "        x, y = gan.genDataset(2000)\n",
    "        cnn.acnn.fit(x,y,batch_size=32, epochs=1, validation_split=0.2, shuffle=True)\n",
    "    score = cnn.acnn.evaluate(x=x_test, y=y_test)\n",
    "    print(f\"{e} score: {score}\")\n",
    "    \n",
    "    prd = cnn.acnn.predict(x=x_test)\n",
    "    prd_x = np.zeros(len(prd))\n",
    "    prd_y = np.zeros(len(prd))\n",
    "    for i in range(len(prd_x)):\n",
    "        prd_x[i] = np.argmax(prd[i])\n",
    "    for i in range(len(prd_y)):\n",
    "        prd_y[i] = np.argmax(y_test[i])\n",
    "    print(f\"F1val: {f1_score(prd_x,prd_y,average='weighted')}\")\n",
    "#\n",
    "prd = cnn.acnn.predict(x=x_test)\n",
    "prd_x = np.zeros(len(prd))\n",
    "prd_y = np.zeros(len(prd))\n",
    "for i in range(len(prd_x)):\n",
    "    prd_x[i] = np.argmax(prd[i])\n",
    "for i in range(len(prd_y)):\n",
    "    prd_y[i] = np.argmax(y_test[i])\n",
    "print(f\"F1val: {f1_score(prd_x,prd_y,average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN CNN WITH MNIST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10608
    },
    "colab_type": "code",
    "id": "soogZLYydhBM",
    "outputId": "7af50bf0-b81e-4a49-dd8b-f5719eeafd6e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((len(x_train),28,28,1))\n",
    "y = np.zeros((len(y_train),10))\n",
    "for i in range(len(y_train)):\n",
    "    y[i][y_train[i]] = 1.0\n",
    "for e in range(10): # epochs\n",
    "    #Train gan\n",
    "    gan.train(x_train[:2000],y_train[:2000], epochs=500, batch=32)\n",
    "    #Train CNN\n",
    "    for i in range(10):\n",
    "        cnn.acnn.fit(x_train[:2000],y[:2000],batch_size=32, epochs=1, validation_split=0.2, shuffle=True)\n",
    "    \n",
    "        gan_x, gan_y = gan.genDataset(2000)\n",
    "        cnn.acnn.fit(gan_x,gan_y,batch_size=32, epochs=1, validation_split=0.2, shuffle=True)\n",
    "    #Test CNN\n",
    "    prd = cnn.acnn.predict(x=x_test)\n",
    "    prd_x = np.zeros(len(prd))\n",
    "    prd_y = np.zeros(len(prd))\n",
    "    for i in range(len(prd_x)):\n",
    "        prd_x[i] = np.argmax(prd[i])\n",
    "    for i in range(len(prd_y)):\n",
    "        prd_y[i] = np.argmax(y_test[i])\n",
    "    print(f\"F1val: {f1_score(prd_x,prd_y,average='weighted')}\")\n",
    "#Final test on CNN\n",
    "score = cnn.acnn.evaluate(x=x_test, y=y_test)\n",
    "print(f\"Final score: {score}\")\n",
    "\n",
    "prd = cnn.acnn.predict(x=x_test)\n",
    "prd_x = np.zeros(len(prd))\n",
    "prd_y = np.zeros(len(prd))\n",
    "for i in range(len(prd_x)):\n",
    "    prd_x[i] = np.argmax(prd[i])\n",
    "for i in range(len(prd_y)):\n",
    "    prd_y[i] = np.argmax(y_test[i])\n",
    "print(f\"F1val: {f1_score(prd_x,prd_y,average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN CNN WITH MNIST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6885
    },
    "colab_type": "code",
    "id": "2n3e4HV0Xqbg",
    "outputId": "dc657129-d4bc-45c1-a184-89d9c4c27784"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((len(x_train),28,28,1))\n",
    "y = np.zeros((len(y_train),10))\n",
    "for i in range(len(y_train)):\n",
    "    y[i][y_train[i]] = 1.0\n",
    "cnn.acnn.fit(x_train[:2000],y[:2000],batch_size=32, epochs=200, validation_split=0.2, shuffle=True)#, callbacks=[metrics])\n",
    "score = cnn.acnn.evaluate(x=x_test, y=y_test)\n",
    "print(f\"Final score: {score}\")\n",
    "\n",
    "prd = cnn.acnn.predict(x=x_test)\n",
    "prd_x = np.zeros(len(prd))\n",
    "prd_y = np.zeros(len(prd))\n",
    "for i in range(len(prd_x)):\n",
    "    prd_x[i] = np.argmax(prd[i])\n",
    "for i in range(len(prd_y)):\n",
    "    prd_y[i] = np.argmax(y_test[i])\n",
    "print(f\"F1val: {f1_score(prd_x,prd_y,average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST CNN SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSZFdclFXSCY"
   },
   "outputs": [],
   "source": [
    "print(f\"Start score: {score}\")\n",
    "\n",
    "prd = cnn.acnn.predict(x=x_test)\n",
    "prd_x = np.zeros(len(prd))\n",
    "prd_y = np.zeros(len(prd))\n",
    "for i in range(len(prd_x)):\n",
    "    prd_x[i] = np.argmax(prd[i])\n",
    "for i in range(len(prd_y)):\n",
    "    prd_y[i] = np.argmax(y_test[i])\n",
    "print(f\"F1val: {f1_score(prd_x,prd_y,average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAh74BmqA9-7"
   },
   "source": [
    "2000 dataset, (fashion MNIST):\n",
    "1. [0.7500727490186692, 0.7989], 0.8022347200020112: 100 epochs (normal)\n",
    "2. [1.3559218359351157, 0.8267], 0.830564980430131: 100 epochs (GAN)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AutoTrainer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
